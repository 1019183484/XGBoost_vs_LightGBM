

{'n_train': 500000, 'max_depth': 5, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 6.767093181610107 seconds
EQBIN_lg: 6.630916595458984 seconds
training LightGBM
LightGBM: 2.7131640911102295 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.675223  0.675223  0.675160
15  0.538657  0.538657  0.535966
30  0.489796  0.489796  0.489301
45  0.461254  0.461254  0.461446
60  0.443328  0.443328  0.440177
75  0.422786  0.422786  0.424996
90  0.407351  0.407351  0.411804

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.675327  0.675327  0.675234
15  0.540042  0.540042  0.537428
30  0.491851  0.491851  0.491172
45  0.463598  0.463598  0.463914
60  0.446067  0.446067  0.443236
75  0.426136  0.426136  0.428417
90  0.411187  0.411187  0.415643

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0         32        32   32
15        32        32   32
30        32        32   32
45        32        32   32
60        30        30   32
75        30        30   30
90        30        30   30

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.132040  0.132040  0.155942
f5   0.077436  0.077436  0.052154
f1   0.074794  0.074794  0.052820
f7   0.069091  0.069091  0.094575
f30  0.068708  0.068708  0.038535


{'n_train': 500000, 'max_depth': 5, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 6.537557363510132 seconds
EQBIN_lg: 6.46275782585144 seconds
training LightGBM
LightGBM: 2.7702364921569824 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.675223  0.675223  0.675160
15  0.538657  0.538657  0.535966
30  0.489796  0.489796  0.489301
45  0.461254  0.461254  0.461446
60  0.443328  0.443328  0.440177
75  0.422786  0.422786  0.424996
90  0.407351  0.407351  0.411804

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.675327  0.675327  0.675234
15  0.540042  0.540042  0.537428
30  0.491851  0.491851  0.491172
45  0.463598  0.463598  0.463914
60  0.446067  0.446067  0.443236
75  0.426136  0.426136  0.428417
90  0.411187  0.411187  0.415643

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0         32        32   32
15        32        32   32
30        32        32   32
45        32        32   32
60        30        30   32
75        30        30   30
90        30        30   30

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.132040  0.132040  0.155942
f5   0.077436  0.077436  0.052154
f1   0.074794  0.074794  0.052820
f7   0.069091  0.069091  0.094575
f30  0.068708  0.068708  0.038535


{'n_train': 500000, 'max_depth': 5, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 6.5610456466674805 seconds
EQBIN_lg: 6.438980579376221 seconds
training LightGBM
LightGBM: 2.9090936183929443 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.675223  0.675223  0.675160
15  0.538657  0.538657  0.535966
30  0.489796  0.489796  0.489301
45  0.461254  0.461254  0.461446
60  0.443328  0.443328  0.440177
75  0.422786  0.422786  0.424996
90  0.407351  0.407351  0.411804

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.675327  0.675327  0.675234
15  0.540042  0.540042  0.537428
30  0.491851  0.491851  0.491172
45  0.463598  0.463598  0.463914
60  0.446067  0.446067  0.443236
75  0.426136  0.426136  0.428417
90  0.411187  0.411187  0.415643

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0         32        32   32
15        32        32   32
30        32        32   32
45        32        32   32
60        30        30   32
75        30        30   30
90        30        30   30

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.132040  0.132040  0.155942
f5   0.077436  0.077436  0.052154
f1   0.074794  0.074794  0.052820
f7   0.069091  0.069091  0.094575
f30  0.068708  0.068708  0.038535


{'n_train': 500000, 'max_depth': 10, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 16.108190536499023 seconds
EQBIN_lg: 16.968087434768677 seconds
training LightGBM
LightGBM: 6.061898231506348 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.660419  0.654859  0.655216
15  0.445656  0.412285  0.411956
30  0.386099  0.343984  0.345766
45  0.345863  0.314970  0.314815
60  0.324551  0.295087  0.297242
75  0.306972  0.278582  0.280252
90  0.294403  0.267757  0.268671

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.660835  0.655368  0.655659
15  0.449799  0.417807  0.417391
30  0.393282  0.353205  0.354650
45  0.355624  0.327243  0.326923
60  0.336659  0.310049  0.311851
75  0.321291  0.296213  0.297484
90  0.310871  0.287915  0.288620

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        256       256  256
15       256       256  256
30       256       256  256
45       256       256  256
60       256       256  256
75       256       256  256
90       256       218  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.132964  0.126935  0.125142
f9   0.075200  0.068042  0.071737
f7   0.067349  0.062007  0.086527
f30  0.064208  0.065658  0.044665
f17  0.061819  0.062047  0.047661


{'n_train': 500000, 'max_depth': 10, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 26.893112421035767 seconds
EQBIN_lg: 26.159011363983154 seconds
training LightGBM
LightGBM: 7.1028525829315186 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.651639  0.651639  0.652016
15  0.390788  0.390788  0.398279
30  0.324340  0.324340  0.326010
45  0.296892  0.296892  0.293489
60  0.274448  0.274448  0.276265
75  0.257760  0.257760  0.260717
90  0.244864  0.244864  0.245533

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.652533  0.652533  0.652829
15  0.401175  0.401175  0.408258
30  0.340391  0.340391  0.341656
45  0.316944  0.316944  0.313490
60  0.298326  0.298326  0.299379
75  0.285349  0.285349  0.287784
90  0.276250  0.276250  0.277091

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        852       852  861
15       745       745  761
30       521       521  613
45       413       413  345
60       723       723  213
75       172       172  655
90       342       342  447

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.129117  0.129117  0.123736
f9   0.069491  0.069491  0.073760
f7   0.063594  0.063594  0.083574
f30  0.063405  0.063405  0.046421
f31  0.062961  0.062961  0.072703


{'n_train': 500000, 'max_depth': 10, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 26.926748752593994 seconds
EQBIN_lg: 26.102295398712158 seconds
training LightGBM
LightGBM: 7.888779163360596 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.651639  0.651639  0.652016
15  0.390788  0.390788  0.398279
30  0.324340  0.324340  0.326010
45  0.296892  0.296892  0.293489
60  0.274448  0.274448  0.276265
75  0.257760  0.257760  0.260717
90  0.244864  0.244864  0.245533

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.652533  0.652533  0.652829
15  0.401175  0.401175  0.408258
30  0.340391  0.340391  0.341656
45  0.316944  0.316944  0.313490
60  0.298326  0.298326  0.299379
75  0.285349  0.285349  0.287784
90  0.276250  0.276250  0.277091

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        852       852  861
15       745       745  761
30       521       521  613
45       413       413  345
60       723       723  213
75       172       172  655
90       342       342  447

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.129117  0.129117  0.123736
f9   0.069491  0.069491  0.073760
f7   0.063594  0.063594  0.083574
f30  0.063405  0.063405  0.046421
f31  0.062961  0.062961  0.072703


{'n_train': 500000, 'max_depth': 15, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 16.304951429367065 seconds
EQBIN_lg: 18.402626037597656 seconds
training LightGBM
LightGBM: 6.744835615158081 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.660419  0.653653  0.654136
15  0.445656  0.396437  0.397010
30  0.386099  0.319544  0.321003
45  0.345863  0.290004  0.291162
60  0.324551  0.271027  0.271467
75  0.307560  0.257728  0.259434
90  0.293811  0.246880  0.251831

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.660835  0.654242  0.654615
15  0.449799  0.402374  0.403389
30  0.393282  0.329378  0.331056
45  0.355624  0.303014  0.304621
60  0.336659  0.287085  0.288107
75  0.321924  0.277238  0.279597
90  0.310479  0.269961  0.275572

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        256       256  256
15       256       256  256
30       256       256  256
45       256       256  256
60       256       256  256
75       256       256  256
90       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.134852  0.122183  0.119956
f9   0.074072  0.068976  0.072055
f7   0.066899  0.059548  0.083330
f30  0.064598  0.066341  0.048558
f17  0.062198  0.052448  0.048202


{'n_train': 500000, 'max_depth': 15, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 40.59304666519165 seconds
EQBIN_lg: 43.81155729293823 seconds
training LightGBM
LightGBM: 12.081468105316162 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.650728  0.643126  0.643280
15  0.381140  0.336316  0.336072
30  0.305366  0.252789  0.253610
45  0.266276  0.218436  0.218422
60  0.239462  0.201520  0.201435
75  0.220427  0.184453  0.184196
90  0.202603  0.173328  0.172655

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.651801  0.644820  0.644745
15  0.393680  0.353735  0.353489
30  0.326315  0.282074  0.282790
45  0.294495  0.258017  0.258081
60  0.274747  0.249367  0.249372
75  0.262881  0.242585  0.242481
90  0.253732  0.240142  0.239978

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       1024      1024  1024
15      1024      1024  1024
30      1024      1024  1024
45      1024      1024  1024
60      1024       817  1024
75      1024      1024  1024
90      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.119489  0.110984  0.112643
f9   0.068468  0.065014  0.065576
f31  0.064456  0.064400  0.071574
f7   0.063498  0.058084  0.078631
f30  0.058822  0.058983  0.047378


{'n_train': 500000, 'max_depth': 15, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 83.95419144630432 seconds
EQBIN_lg: 86.73566222190857 seconds
training LightGBM
LightGBM: 19.53372025489807 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.638249  0.635119  0.635476
15  0.304004  0.287983  0.288254
30  0.210452  0.192443  0.195039
45  0.178598  0.161888  0.163047
60  0.161279  0.149479  0.149784
75  0.144680  0.133556  0.132235
90  0.131739  0.119178  0.120430

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.641995  0.639688  0.639633
15  0.342940  0.332860  0.332248
30  0.272719  0.263850  0.264316
45  0.252559  0.245412  0.244925
60  0.243413  0.239214  0.238800
75  0.237795  0.232962  0.233838
90  0.234783  0.229043  0.230730

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       4096      4096  4096
15      4096      4096  4096
30      3185      3649  3322
45      1699      2252  2027
60      1362       978   742
75       430       584  2094
90      4096       516  1156

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.109621  0.106498  0.107152
f9   0.064802  0.062458  0.062443
f31  0.063291  0.062963  0.074507
f7   0.057576  0.057145  0.072371
f30  0.056391  0.055450  0.047487


{'n_train': 500000, 'max_depth': 20, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 16.036890983581543 seconds
EQBIN_lg: 17.49635672569275 seconds
training LightGBM
LightGBM: 7.070460557937622 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.660419  0.653653  0.654136
15  0.445656  0.395474  0.396547
30  0.386099  0.317459  0.318116
45  0.345863  0.283433  0.283366
60  0.324551  0.263905  0.264350
75  0.307560  0.253387  0.253675
90  0.293811  0.245690  0.245291

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.660835  0.654242  0.654615
15  0.449799  0.401607  0.402790
30  0.393282  0.327482  0.328341
45  0.355624  0.296832  0.296807
60  0.336659  0.280721  0.281162
75  0.321924  0.273877  0.274104
90  0.310479  0.270201  0.269665

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        256       256  256
15       256       256  256
30       256       256  256
45       256       256  256
60       256       256  256
75       256       256  256
90       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.134852  0.122711  0.118981
f9   0.074072  0.070080  0.073013
f7   0.066899  0.060588  0.083736
f30  0.064598  0.064594  0.048376
f17  0.062198  0.053926  0.048596


{'n_train': 500000, 'max_depth': 20, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 40.72309899330139 seconds
EQBIN_lg: 45.36574101448059 seconds
training LightGBM
LightGBM: 12.961489915847778 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.650728  0.642855  0.643032
15  0.381140  0.332252  0.332020
30  0.305366  0.246955  0.246237
45  0.266276  0.211213  0.209098
60  0.239386  0.189230  0.189731
75  0.219788  0.174473  0.173735
90  0.205906  0.161722  0.160602

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.651801  0.644539  0.644554
15  0.393680  0.350114  0.349836
30  0.326315  0.277369  0.276494
45  0.294495  0.252317  0.250577
60  0.274656  0.242146  0.241890
75  0.262760  0.238951  0.237593
90  0.257027  0.236805  0.235324

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       1024      1024  1024
15      1024      1024  1024
30      1024      1024  1024
45      1024      1024  1024
60      1024      1024  1024
75      1024      1024  1024
90      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.121363  0.110317  0.109626
f9   0.067708  0.064792  0.066448
f7   0.063576  0.057961  0.077083
f31  0.063429  0.064894  0.070693
f10  0.057573  0.052860  0.092385


{'n_train': 500000, 'max_depth': 20, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 111.00133061408997 seconds
EQBIN_lg: 112.69759225845337 seconds
training LightGBM
LightGBM: 26.690208435058594 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.638249  0.632386  0.632509
15  0.304004  0.263888  0.264186
30  0.208051  0.162379  0.162807
45  0.159551  0.120920  0.120926
60  0.139336  0.104461  0.104047
75  0.118094  0.089495  0.087457
90  0.098598  0.073592  0.073990

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.641995  0.637384  0.637262
15  0.342940  0.317612  0.317334
30  0.271828  0.249819  0.249884
45  0.244324  0.230677  0.230547
60  0.235702  0.225052  0.224974
75  0.230296  0.222071  0.221529
90  0.226332  0.219748  0.220057

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       4096      4096  4096
15      4096      4096  4096
30      4096      4096  4096
45      4096      2562  3954
60      3032      4096  4096
75      4096      2808  1049
90      4096      4096  4096

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f21  0.102452  0.099413  0.102236
f9   0.063442  0.062268  0.059701
f31  0.062921  0.063553  0.071042
f7   0.057335  0.055896  0.071341
f10  0.055503  0.053120  0.083647


{'n_train': 1000000, 'max_depth': 5, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 12.232143878936768 seconds
EQBIN_lg: 11.66616678237915 seconds
training LightGBM
LightGBM: 5.713316202163696 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.670712  0.670712  0.670705
15  0.519764  0.519764  0.518376
30  0.465616  0.465616  0.463697
45  0.437321  0.437321  0.437981
60  0.416064  0.416064  0.415917
75  0.401604  0.401604  0.400009
90  0.387141  0.387141  0.388828

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.670815  0.670815  0.670768
15  0.520467  0.520467  0.518830
30  0.466785  0.466785  0.464736
45  0.439002  0.439002  0.439353
60  0.418094  0.418094  0.417748
75  0.403941  0.403941  0.402137
90  0.389714  0.389714  0.391165

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0         32        32   32
15        32        32   32
30        32        32   32
45        32        32   32
60        32        32   32
75        32        32   32
90        29        29   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.145318  0.145318  0.141575
f15  0.116695  0.116695  0.128330
f27  0.095922  0.095922  0.075540
f26  0.080331  0.080331  0.089207
f12  0.073030  0.073030  0.064558


{'n_train': 1000000, 'max_depth': 5, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 12.191505908966064 seconds
EQBIN_lg: 11.627804279327393 seconds
training LightGBM
LightGBM: 5.703240394592285 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.670712  0.670712  0.670705
15  0.519764  0.519764  0.518376
30  0.465616  0.465616  0.463697
45  0.437321  0.437321  0.437981
60  0.416064  0.416064  0.415917
75  0.401604  0.401604  0.400009
90  0.387141  0.387141  0.388828

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.670815  0.670815  0.670768
15  0.520467  0.520467  0.518830
30  0.466785  0.466785  0.464736
45  0.439002  0.439002  0.439353
60  0.418094  0.418094  0.417748
75  0.403941  0.403941  0.402137
90  0.389714  0.389714  0.391165

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0         32        32   32
15        32        32   32
30        32        32   32
45        32        32   32
60        32        32   32
75        32        32   32
90        29        29   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.145318  0.145318  0.141575
f15  0.116695  0.116695  0.128330
f27  0.095922  0.095922  0.075540
f26  0.080331  0.080331  0.089207
f12  0.073030  0.073030  0.064558


{'n_train': 1000000, 'max_depth': 5, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 11.797897815704346 seconds
EQBIN_lg: 11.816361427307129 seconds
training LightGBM
LightGBM: 5.860211133956909 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.670712  0.670712  0.670705
15  0.519764  0.519764  0.518376
30  0.465616  0.465616  0.463697
45  0.437321  0.437321  0.437981
60  0.416064  0.416064  0.415917
75  0.401604  0.401604  0.400009
90  0.387141  0.387141  0.388828

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.670815  0.670815  0.670768
15  0.520467  0.520467  0.518830
30  0.466785  0.466785  0.464736
45  0.439002  0.439002  0.439353
60  0.418094  0.418094  0.417748
75  0.403941  0.403941  0.402137
90  0.389714  0.389714  0.391165

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0         32        32   32
15        32        32   32
30        32        32   32
45        32        32   32
60        32        32   32
75        32        32   32
90        29        29   32

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.145318  0.145318  0.141575
f15  0.116695  0.116695  0.128330
f27  0.095922  0.095922  0.075540
f26  0.080331  0.080331  0.089207
f12  0.073030  0.073030  0.064558


{'n_train': 1000000, 'max_depth': 10, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 23.732237100601196 seconds
EQBIN_lg: 25.28367853164673 seconds
training LightGBM
LightGBM: 12.487559080123901 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.658978  0.654731  0.654717
15  0.436275  0.409136  0.409312
30  0.364068  0.335276  0.334841
45  0.330131  0.300765  0.302416
60  0.309972  0.284512  0.283367
75  0.294529  0.270522  0.272764
90  0.285495  0.258248  0.259036

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.659296  0.655093  0.655052
15  0.439298  0.412880  0.412920
30  0.369188  0.341609  0.340894
45  0.336710  0.308982  0.310120
60  0.317876  0.294392  0.292701
75  0.303677  0.281850  0.283464
90  0.295721  0.271192  0.271255

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        256       256  256
15       256       256  256
30       256       256  256
45       256       256  256
60       256       256  256
75       256       256  256
90       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.122722  0.123735  0.111525
f15  0.118576  0.112243  0.100826
f27  0.100339  0.093356  0.061284
f26  0.082033  0.077034  0.074265
f12  0.066887  0.067589  0.053701


{'n_train': 1000000, 'max_depth': 10, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 39.907124042510986 seconds
EQBIN_lg: 39.01840353012085 seconds
training LightGBM
LightGBM: 14.837634325027466 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.651142  0.651142  0.651262
15  0.383524  0.383524  0.382883
30  0.308016  0.308016  0.306793
45  0.274393  0.274393  0.270757
60  0.258309  0.258309  0.256461
75  0.246340  0.246340  0.244974
90  0.234777  0.234777  0.231011

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.651907  0.651907  0.652014
15  0.391024  0.391024  0.390332
30  0.320003  0.320003  0.318776
45  0.289681  0.289681  0.286168
60  0.275817  0.275817  0.273881
75  0.265776  0.265776  0.264431
90  0.256561  0.256561  0.253221

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        921       921  920
15       887       887  893
30       853       853  873
45       605       605  522
60       606       606  411
75       807       807  749
90       524       524  405

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.118859  0.118859  0.109663
f15  0.109684  0.109684  0.096241
f27  0.091682  0.091682  0.058649
f26  0.079292  0.079292  0.071698
f12  0.062056  0.062056  0.053927


{'n_train': 1000000, 'max_depth': 10, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 39.01489853858948 seconds
EQBIN_lg: 39.089027404785156 seconds
training LightGBM
LightGBM: 15.520090818405151 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.651142  0.651142  0.651262
15  0.383524  0.383524  0.382883
30  0.308016  0.308016  0.306793
45  0.274393  0.274393  0.270757
60  0.258309  0.258309  0.256461
75  0.246340  0.246340  0.244974
90  0.234777  0.234777  0.231011

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.651907  0.651907  0.652014
15  0.391024  0.391024  0.390332
30  0.320003  0.320003  0.318776
45  0.289681  0.289681  0.286168
60  0.275817  0.275817  0.273881
75  0.265776  0.265776  0.264431
90  0.256561  0.256561  0.253221

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        921       921  920
15       887       887  893
30       853       853  873
45       605       605  522
60       606       606  411
75       807       807  749
90       524       524  405

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.118859  0.118859  0.109663
f15  0.109684  0.109684  0.096241
f27  0.091682  0.091682  0.058649
f26  0.079292  0.079292  0.071698
f12  0.062056  0.062056  0.053927


{'n_train': 1000000, 'max_depth': 15, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 23.348346710205078 seconds
EQBIN_lg: 27.049471616744995 seconds
training LightGBM
LightGBM: 14.595597505569458 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.658978  0.653759  0.653808
15  0.436275  0.397689  0.398117
30  0.364068  0.319342  0.319973
45  0.330131  0.283043  0.283668
60  0.309972  0.265661  0.264003
75  0.294529  0.251078  0.250236
90  0.285495  0.242280  0.240422

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.659296  0.654160  0.654163
15  0.439298  0.401931  0.402156
30  0.369188  0.325920  0.326448
45  0.336710  0.291684  0.292018
60  0.317876  0.276088  0.274093
75  0.303677  0.263207  0.261897
90  0.295721  0.256162  0.253919

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        256       256  256
15       256       256  256
30       256       256  256
45       256       256  256
60       256       256  256
75       256       256  256
90       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.122722  0.124221  0.109242
f15  0.118576  0.109842  0.099960
f27  0.100339  0.091437  0.061321
f26  0.082033  0.069572  0.072512
f12  0.066887  0.067958  0.050613


{'n_train': 1000000, 'max_depth': 15, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 51.174288749694824 seconds
EQBIN_lg: 55.96244788169861 seconds
training LightGBM
LightGBM: 22.36334776878357 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.650549  0.643973  0.643964
15  0.379917  0.338724  0.339573
30  0.301987  0.258629  0.258288
45  0.263452  0.222105  0.222847
60  0.243268  0.206296  0.204282
75  0.227790  0.192565  0.190924
90  0.212702  0.181020  0.180098

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.651348  0.644941  0.644912
15  0.388126  0.349592  0.350434
30  0.315416  0.276035  0.275638
45  0.281390  0.245225  0.246022
60  0.264951  0.234128  0.232542
75  0.253319  0.226688  0.225360
90  0.242329  0.221518  0.221433

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       1024      1024  1024
15      1024      1024  1024
30      1024      1024  1024
45      1024      1024  1024
60      1024       988  1024
75      1024      1024  1024
90      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.114412  0.121463  0.101261
f15  0.108761  0.106407  0.095283
f27  0.087270  0.083703  0.055689
f26  0.074391  0.067210  0.067430
f12  0.062046  0.061673  0.052679


{'n_train': 1000000, 'max_depth': 15, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 118.16458559036255 seconds
EQBIN_lg: 121.37146043777466 seconds
training LightGBM
LightGBM: 34.58445334434509 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.640997  0.636590  0.636622
15  0.317563  0.291948  0.292186
30  0.229187  0.202548  0.203606
45  0.185212  0.164273  0.164485
60  0.167143  0.148091  0.147720
75  0.152685  0.131793  0.136119
90  0.137441  0.122523  0.126096

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.643100  0.639137  0.639139
15  0.340330  0.319891  0.320047
30  0.266388  0.248607  0.249663
45  0.235921  0.225026  0.225425
60  0.225254  0.216929  0.216980
75  0.219214  0.211246  0.212986
90  0.214868  0.209586  0.210982

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       4096      4096  4096
15      4096      4096  4096
30      4096      4096  4096
45      4096      4096  3423
60      1158      1079  1274
75      4096      2594  4096
90      4096      4096  3078

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.105008  0.109780  0.097533
f15  0.096645  0.094375  0.088551
f27  0.077698  0.075999  0.053693
f26  0.066509  0.065021  0.061786
f31  0.057554  0.055457  0.077371


{'n_train': 1000000, 'max_depth': 20, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 23.039791584014893 seconds
EQBIN_lg: 26.15120816230774 seconds
training LightGBM
LightGBM: 14.9300856590271 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.658978  0.653759  0.653800
15  0.436275  0.397949  0.398117
30  0.364068  0.319321  0.319184
45  0.330131  0.280924  0.281330
60  0.309972  0.260308  0.260469
75  0.294529  0.246195  0.247937
90  0.285495  0.238251  0.238835

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.659296  0.654160  0.654152
15  0.439298  0.402164  0.402135
30  0.369188  0.326011  0.325680
45  0.336710  0.289500  0.289674
60  0.317876  0.270695  0.270578
75  0.303677  0.258203  0.259780
90  0.295721  0.252180  0.252420

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        256       256  256
15       256       256  256
30       256       256  256
45       256       256  256
60       256       256  256
75       256       256  256
90       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.122722  0.127496  0.108261
f15  0.118576  0.115520  0.100772
f27  0.100339  0.085996  0.059478
f26  0.082033  0.069905  0.073138
f12  0.066887  0.064759  0.050589


{'n_train': 1000000, 'max_depth': 20, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 51.3400022983551 seconds
EQBIN_lg: 57.227585315704346 seconds
training LightGBM
LightGBM: 23.48706865310669 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.650549  0.643458  0.643544
15  0.379917  0.337132  0.336834
30  0.301987  0.254547  0.254986
45  0.263452  0.216717  0.216809
60  0.243268  0.196326  0.195149
75  0.227790  0.183981  0.183190
90  0.212702  0.173867  0.173329

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.651348  0.644455  0.644459
15  0.388126  0.348313  0.347928
30  0.315416  0.272545  0.272969
45  0.281390  0.240839  0.240897
60  0.264951  0.226687  0.225333
75  0.253319  0.221760  0.221045
90  0.242329  0.218880  0.218284

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       1024      1024  1024
15      1024      1024  1024
30      1024      1024  1024
45      1024      1024  1024
60      1024      1024  1024
75      1024      1024  1024
90      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.114391  0.124148  0.100763
f15  0.108787  0.105787  0.093403
f27  0.087268  0.079576  0.054888
f26  0.074383  0.064704  0.067239
f12  0.062046  0.057875  0.052051


{'n_train': 1000000, 'max_depth': 20, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 136.7069935798645 seconds
EQBIN_lg: 152.03030395507812 seconds
training LightGBM
LightGBM: 44.35920190811157 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.640997  0.633776  0.633869
15  0.317563  0.277907  0.278134
30  0.229187  0.185827  0.185784
45  0.185212  0.143685  0.144548
60  0.157450  0.119812  0.120787
75  0.139900  0.103525  0.104295
90  0.124637  0.089445  0.090709

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.643100  0.636476  0.636632
15  0.340330  0.308290  0.308505
30  0.266388  0.237060  0.237034
45  0.235921  0.214238  0.214963
60  0.220722  0.206273  0.206470
75  0.215195  0.203469  0.203762
90  0.211442  0.201209  0.201729

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       4096      4096  4096
15      4096      4096  4096
30      4096      4096  4096
45      4096      4096  4096
60      4096      3702  4088
75      4096      3173  4096
90      4096      2853  1553

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f23  0.099783  0.108317  0.093926
f15  0.094605  0.092356  0.085839
f27  0.076620  0.070463  0.052364
f26  0.065300  0.063169  0.060294
f31  0.057652  0.054738  0.076526


{'n_train': 2000000, 'max_depth': 5, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 21.535839557647705 seconds
EQBIN_lg: 21.87316632270813 seconds
training LightGBM
LightGBM: 11.556857824325562 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.668197  0.668197  0.668188
15  0.502499  0.502499  0.503965
30  0.458417  0.458417  0.458402
45  0.428591  0.428591  0.434806
60  0.412420  0.412420  0.415474
75  0.397671  0.397671  0.401653
90  0.387534  0.387534  0.387192

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.668257  0.668257  0.668256
15  0.503355  0.503355  0.504954
30  0.459662  0.459662  0.459852
45  0.430273  0.430273  0.436487
60  0.414352  0.414352  0.417379
75  0.399917  0.399917  0.403774
90  0.389928  0.389928  0.389481

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0         32        32   32
15        32        32   32
30        32        32   32
45        32        32   32
60        32        32   32
75        31        31   32
90        27        27   31

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.132506  0.132506  0.123768
f29  0.096810  0.096810  0.102832
f16  0.089042  0.089042  0.085066
f31  0.081461  0.081461  0.044004
f12  0.069586  0.069586  0.059194


{'n_train': 2000000, 'max_depth': 5, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 22.038800716400146 seconds
EQBIN_lg: 21.99045753479004 seconds
training LightGBM
LightGBM: 11.537817478179932 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.668197  0.668197  0.668188
15  0.502499  0.502499  0.503965
30  0.458417  0.458417  0.458402
45  0.428591  0.428591  0.434806
60  0.412420  0.412420  0.415474
75  0.397671  0.397671  0.401653
90  0.387534  0.387534  0.387192

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.668257  0.668257  0.668256
15  0.503355  0.503355  0.504954
30  0.459662  0.459662  0.459852
45  0.430273  0.430273  0.436487
60  0.414352  0.414352  0.417379
75  0.399917  0.399917  0.403774
90  0.389928  0.389928  0.389481

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0         32        32   32
15        32        32   32
30        32        32   32
45        32        32   32
60        32        32   32
75        31        31   32
90        27        27   31

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.132506  0.132506  0.123768
f29  0.096810  0.096810  0.102832
f16  0.089042  0.089042  0.085066
f31  0.081461  0.081461  0.044004
f12  0.069586  0.069586  0.059194


{'n_train': 2000000, 'max_depth': 5, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 22.205700159072876 seconds
EQBIN_lg: 22.258994340896606 seconds
training LightGBM
LightGBM: 11.644569635391235 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.668197  0.668197  0.668188
15  0.502499  0.502499  0.503965
30  0.458417  0.458417  0.458402
45  0.428591  0.428591  0.434806
60  0.412420  0.412420  0.415474
75  0.397671  0.397671  0.401653
90  0.387534  0.387534  0.387192

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.668257  0.668257  0.668256
15  0.503355  0.503355  0.504954
30  0.459662  0.459662  0.459852
45  0.430273  0.430273  0.436487
60  0.414352  0.414352  0.417379
75  0.399917  0.399917  0.403774
90  0.389928  0.389928  0.389481

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0         32        32   32
15        32        32   32
30        32        32   32
45        32        32   32
60        32        32   32
75        31        31   32
90        27        27   31

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.132506  0.132506  0.123768
f29  0.096810  0.096810  0.102832
f16  0.089042  0.089042  0.085066
f31  0.081461  0.081461  0.044004
f12  0.069586  0.069586  0.059194


{'n_train': 2000000, 'max_depth': 10, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 36.746057748794556 seconds
EQBIN_lg: 41.52183938026428 seconds
training LightGBM
LightGBM: 25.16833209991455 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.656158  0.652710  0.652792
15  0.416301  0.395077  0.392429
30  0.351350  0.320231  0.321848
45  0.323185  0.296724  0.296307
60  0.307805  0.279908  0.278490
75  0.297070  0.263889  0.262328
90  0.282135  0.251113  0.251554

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.656404  0.653028  0.653045
15  0.419014  0.398354  0.395641
30  0.355150  0.324895  0.326547
45  0.327888  0.302424  0.302021
60  0.313217  0.286330  0.285103
75  0.303062  0.270972  0.269749
90  0.288764  0.258979  0.259703

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        256       256  256
15       256       256  256
30       256       256  256
45       256       256  256
60       256       256  256
75       256       256  256
90       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.143172  0.151235  0.106190
f16  0.102192  0.101023  0.081293
f29  0.101603  0.099372  0.091698
f24  0.068971  0.063669  0.127898
f9   0.065345  0.062849  0.074307


{'n_train': 2000000, 'max_depth': 10, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 61.20344281196594 seconds
EQBIN_lg: 60.14697265625 seconds
training LightGBM
LightGBM: 30.418845176696777 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.648192  0.648192  0.648349
15  0.364879  0.364879  0.365444
30  0.291926  0.291926  0.290530
45  0.270327  0.270327  0.264973
60  0.251356  0.251356  0.250603
75  0.239048  0.239048  0.236150
90  0.223576  0.223576  0.221929

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.648686  0.648686  0.648817
15  0.370408  0.370408  0.370903
30  0.300284  0.300284  0.298680
45  0.280182  0.280182  0.274822
60  0.262590  0.262590  0.261667
75  0.251719  0.251719  0.248570
90  0.237951  0.237951  0.235994

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        985       985  989
15       985       985  980
30       858       858  850
45       545       545  583
60       428       428  431
75       841       841  714
90       819       819  442

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.148919  0.148919  0.102556
f29  0.099831  0.099831  0.091464
f16  0.096085  0.096085  0.079591
f24  0.069015  0.069015  0.125335
f8   0.061136  0.061136  0.089570


{'n_train': 2000000, 'max_depth': 10, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 60.94802188873291 seconds
EQBIN_lg: 60.71748471260071 seconds
training LightGBM
LightGBM: 30.569921731948853 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.648192  0.648192  0.648349
15  0.364879  0.364879  0.365444
30  0.291926  0.291926  0.290530
45  0.270327  0.270327  0.264973
60  0.251356  0.251356  0.250603
75  0.239048  0.239048  0.236150
90  0.223576  0.223576  0.221929

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.648686  0.648686  0.648817
15  0.370408  0.370408  0.370903
30  0.300284  0.300284  0.298680
45  0.280182  0.280182  0.274822
60  0.262590  0.262590  0.261667
75  0.251719  0.251719  0.248570
90  0.237951  0.237951  0.235994

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        985       985  989
15       985       985  980
30       858       858  850
45       545       545  583
60       428       428  431
75       841       841  714
90       819       819  442

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.148919  0.148919  0.102556
f29  0.099831  0.099831  0.091464
f16  0.096085  0.096085  0.079591
f24  0.069015  0.069015  0.125335
f8   0.061136  0.061136  0.089570


{'n_train': 2000000, 'max_depth': 15, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 35.84563946723938 seconds
EQBIN_lg: 43.19354033470154 seconds
training LightGBM
LightGBM: 29.075278520584106 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.656158  0.652332  0.652412
15  0.416301  0.389403  0.390274
30  0.351350  0.310204  0.310276
45  0.323185  0.279427  0.279539
60  0.307805  0.257062  0.260162
75  0.297070  0.242686  0.244636
90  0.282135  0.233639  0.235159

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.656404  0.652657  0.652722
15  0.419014  0.392599  0.393377
30  0.355150  0.314945  0.314977
45  0.327888  0.285217  0.285360
60  0.313217  0.263842  0.266816
75  0.303062  0.250386  0.252220
90  0.288764  0.242166  0.243526

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        256       256  256
15       256       256  256
30       256       256  256
45       256       256  256
60       256       256  256
75       256       256  256
90       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.143172  0.150542  0.103823
f16  0.102192  0.098082  0.080625
f29  0.101603  0.101039  0.090142
f24  0.068971  0.063456  0.126714
f9   0.065345  0.061063  0.073572


{'n_train': 2000000, 'max_depth': 15, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 71.4542784690857 seconds
EQBIN_lg: 78.88795614242554 seconds
training LightGBM
LightGBM: 42.99672842025757 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.647957  0.642051  0.642409
15  0.364843  0.329033  0.328655
30  0.291531  0.247037  0.248138
45  0.262601  0.218827  0.219523
60  0.241492  0.201040  0.202016
75  0.223358  0.188844  0.185918
90  0.208667  0.178791  0.174752

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.648459  0.642706  0.643083
15  0.370401  0.335989  0.335617
30  0.300153  0.257969  0.258888
45  0.273498  0.232734  0.233236
60  0.254403  0.217737  0.218356
75  0.238466  0.208457  0.205323
90  0.225990  0.201775  0.197384

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       1024      1024  1024
15      1024      1024  1024
30      1024      1024  1024
45      1024      1024  1024
60      1024      1024  1024
75      1024      1024  1024
90      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.136895  0.150840  0.097713
f29  0.097251  0.091536  0.087613
f16  0.091364  0.090726  0.077694
f24  0.070825  0.067282  0.124382
f8   0.063158  0.054615  0.091636


{'n_train': 2000000, 'max_depth': 15, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 157.61267971992493 seconds
EQBIN_lg: 161.88291907310486 seconds
training LightGBM
LightGBM: 61.20999097824097 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.639568  0.633948  0.634346
15  0.313271  0.282930  0.283375
30  0.233586  0.199557  0.198569
45  0.198225  0.166658  0.168995
60  0.177924  0.150070  0.153371
75  0.156173  0.138522  0.137845
90  0.143805  0.130463  0.130974

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.640793  0.635462  0.635917
15  0.326588  0.299185  0.299810
30  0.254553  0.225802  0.224973
45  0.226202  0.201993  0.203970
60  0.211672  0.191832  0.194245
75  0.196742  0.186291  0.186401
90  0.190958  0.184805  0.184626

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       4096      4096  4096
15      4096      4096  4096
30      4096      4096  4096
45      4096      4096  4096
60      4096      4096  4096
75      4096      1622  4096
90      4096      4096  4096

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.125213  0.127265  0.093432
f29  0.089759  0.087484  0.084188
f16  0.086290  0.085621  0.074796
f24  0.074036  0.070726  0.120656
f8   0.061979  0.058866  0.090031


{'n_train': 2000000, 'max_depth': 20, 'num_leaves': 256}
training XGBoost
EQBIN_dw: 37.557276010513306 seconds
EQBIN_lg: 43.41077256202698 seconds
training LightGBM
LightGBM: 29.2593891620636 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.656158  0.652332  0.652412
15  0.416301  0.390072  0.389121
30  0.351350  0.307483  0.307653
45  0.323185  0.271729  0.272584
60  0.307805  0.250788  0.252946
75  0.297070  0.239523  0.241339
90  0.282135  0.230279  0.231536

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.656404  0.652657  0.652722
15  0.419014  0.393276  0.392238
30  0.355150  0.312305  0.312605
45  0.327888  0.277716  0.278590
60  0.313217  0.257691  0.259908
75  0.303062  0.247327  0.249213
90  0.288764  0.238973  0.240280

Leaf counts
    EQBIN_dw  EQBIN_lg  LGB
0        256       256  256
15       256       256  256
30       256       256  256
45       256       256  256
60       256       256  256
75       256       256  256
90       256       256  256

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.143172  0.149127  0.103766
f16  0.102192  0.096898  0.080397
f29  0.101603  0.097304  0.088771
f24  0.068971  0.063863  0.130201
f9   0.065345  0.061909  0.073858


{'n_train': 2000000, 'max_depth': 20, 'num_leaves': 1024}
training XGBoost
EQBIN_dw: 70.56397914886475 seconds
EQBIN_lg: 78.70763897895813 seconds
training LightGBM
LightGBM: 43.53632140159607 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.647957  0.641910  0.642232
15  0.364843  0.327983  0.326803
30  0.291531  0.242923  0.243307
45  0.262601  0.209299  0.209499
60  0.241492  0.189310  0.189209
75  0.223358  0.178215  0.177048
90  0.208667  0.172201  0.170584

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.648459  0.642562  0.642901
15  0.370401  0.335102  0.333754
30  0.300153  0.253984  0.254423
45  0.273498  0.223411  0.223760
60  0.254403  0.206357  0.206362
75  0.238466  0.198666  0.197455
90  0.225990  0.196271  0.194991

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       1024      1024  1024
15      1024      1024  1024
30      1024      1024  1024
45      1024      1024  1024
60      1024      1024  1024
75      1024      1024  1024
90      1024      1024  1024

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.136895  0.150626  0.096939
f29  0.097251  0.095169  0.086938
f16  0.091364  0.087314  0.077180
f24  0.070825  0.066569  0.124171
f8   0.063158  0.055286  0.091821


{'n_train': 2000000, 'max_depth': 20, 'num_leaves': 4096}
training XGBoost
EQBIN_dw: 164.1555461883545 seconds
EQBIN_lg: 184.8951132297516 seconds
training LightGBM
LightGBM: 71.80288219451904 seconds

binary_logloss train
    EQBIN_dw  EQBIN_lg       LGB
0   0.639568  0.632341  0.632666
15  0.313271  0.273735  0.273322
30  0.233586  0.186708  0.185858
45  0.198093  0.152459  0.150779
60  0.173090  0.133472  0.132900
75  0.153095  0.118241  0.119241
90  0.139472  0.107197  0.107817

binary_logloss valid
    EQBIN_dw  EQBIN_lg       LGB
0   0.640793  0.633997  0.634296
15  0.326588  0.291041  0.290672
30  0.254553  0.214966  0.214074
45  0.226137  0.191327  0.189893
60  0.207948  0.182071  0.181481
75  0.195296  0.177586  0.178609
90  0.188616  0.175777  0.176168

Leaf counts
    EQBIN_dw  EQBIN_lg   LGB
0       4096      4096  4096
15      4096      4096  4096
30      4096      4096  4096
45      4096      4096  4096
60      4096      4096  4096
75      4096      4096  4096
90      4096      3821  4096

Feature importance(gain) sorted by EQBIN_dw
     EQBIN_dw  EQBIN_lg       LGB
f3   0.123389  0.124942  0.091479
f29  0.089330  0.086883  0.082028
f16  0.086000  0.081665  0.074096
f24  0.075817  0.068495  0.116581
f8   0.061829  0.056723  0.089787


                             Time(sec)                       Ratio  \
                              EQBIN_dw EQBIN_lg   LGB EQBIN_dw/LGB   
n_train max_depth num_leaves                                         
500000  5         256              6.8      6.6   2.7          2.5   
                  1024             6.5      6.5   2.8          2.4   
                  4096             6.6      6.4   2.9          2.3   
        10        256             16.1     17.0   6.1          2.7   
                  1024            26.9     26.2   7.1          3.8   
                  4096            26.9     26.1   7.9          3.4   
        15        256             16.3     18.4   6.7          2.4   
                  1024            40.6     43.8  12.1          3.4   
                  4096            84.0     86.7  19.5          4.3   
        20        256             16.0     17.5   7.1          2.3   
                  1024            40.7     45.4  13.0          3.1   
                  4096           111.0    112.7  26.7          4.2   
1000000 5         256             12.2     11.7   5.7          2.1   
                  1024            12.2     11.6   5.7          2.1   
                  4096            11.8     11.8   5.9          2.0   
        10        256             23.7     25.3  12.5          1.9   
                  1024            39.9     39.0  14.8          2.7   
                  4096            39.0     39.1  15.5          2.5   
        15        256             23.3     27.0  14.6          1.6   
                  1024            51.2     56.0  22.4          2.3   
                  4096           118.2    121.4  34.6          3.4   
        20        256             23.0     26.2  14.9          1.5   
                  1024            51.3     57.2  23.5          2.2   
                  4096           136.7    152.0  44.4          3.1   
2000000 5         256             21.5     21.9  11.6          1.9   
                  1024            22.0     22.0  11.5          1.9   
                  4096            22.2     22.3  11.6          1.9   
        10        256             36.7     41.5  25.2          1.5   
                  1024            61.2     60.1  30.4          2.0   
                  4096            60.9     60.7  30.6          2.0   
        15        256             35.8     43.2  29.1          1.2   
                  1024            71.5     78.9  43.0          1.7   
                  4096           157.6    161.9  61.2          2.6   
        20        256             37.6     43.4  29.3          1.3   
                  1024            70.6     78.7  43.5          1.6   
                  4096           164.2    184.9  71.8          2.3   

                                           
                             EQBIN_lg/LGB  
n_train max_depth num_leaves               
500000  5         256                 2.4  
                  1024                2.3  
                  4096                2.2  
        10        256                 2.8  
                  1024                3.7  
                  4096                3.3  
        15        256                 2.7  
                  1024                3.6  
                  4096                4.4  
        20        256                 2.5  
                  1024                3.5  
                  4096                4.2  
1000000 5         256                 2.0  
                  1024                2.0  
                  4096                2.0  
        10        256                 2.0  
                  1024                2.6  
                  4096                2.5  
        15        256                 1.9  
                  1024                2.5  
                  4096                3.5  
        20        256                 1.8  
                  1024                2.4  
                  4096                3.4  
2000000 5         256                 1.9  
                  1024                1.9  
                  4096                1.9  
        10        256                 1.6  
                  1024                2.0  
                  4096                2.0  
        15        256                 1.5  
                  1024                1.8  
                  4096                2.6  
        20        256                 1.5  
                  1024                1.8  
                  4096                2.6  

Done: 4412.332545042038 seconds
